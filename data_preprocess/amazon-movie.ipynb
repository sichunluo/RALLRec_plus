{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from transformers import set_seed\n",
    "import hashlib\n",
    "import json\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "dataset_name = \"amazon-movies\"\n",
    "root = f\"../data/{dataset_name}\"\n",
    "source_dir = os.path.join(root, \"raw_data\")\n",
    "target_dir = os.path.join(root, \"proc_data\")\n",
    "\n",
    "os.makedirs(target_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = []\n",
    "with open(os.path.join(source_dir, \"meta_Movies_and_TV.json\"), 'r') as fp:\n",
    "    for line in fp:\n",
    "        ele = json.loads(line.strip())\n",
    "        obj.append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203766\n",
      "{'category': ['Movies & TV', 'Genre for Featured Categories', 'Kids & Family'], 'tech1': '', 'description': ['The romantic drama Wait Your Turn introduces us to Thad MacArthur and Eve Cassidy, a couple who must decide whether they can put their rocky past behind them or if their story will end in sad repetition. When Thad unexpectedly comes back into Eve\\'s life after breaking her heart in college, he discovers a changed woman. After much soul-searching, Eve has decided she\\'s going to \"wait\" until marriage. Now, Thad must decide if he\\'s the man who can honor her decision. Unfortunately, Thad and Eve aren\\'t the only couple facing difficult decisions. Matt and Liza confront a mistake from college that will affect their lives forever, while Evan and Belinda must determine if they are compatible for life. As each couple juggles the complications of love and life, they realize the right person is out there if they simply wait their turn.', \"<i>This product is manufactured on demand using DVD-R recordable media. Amazon.com's standard return policy will apply.</i>\", 'A great Movie! --Jerome Wiilhort - Celebrity Video'], 'fit': '', 'title': 'Wait Your Turn', 'also_buy': ['0740319191'], 'tech2': '', 'brand': 'Josh Murray', 'feature': [], 'rank': '291,886 in Movies & TV (', 'also_view': [], 'main_cat': 'Movies & TV', 'similar_item': '', 'date': '', 'price': '$5.08', 'asin': '0740318543', 'imageURL': [], 'imageURLHighRes': []}\n"
     ]
    }
   ],
   "source": [
    "print(len(obj))\n",
    "print(obj[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1 = []\n",
    "cnt = 0\n",
    "with open(os.path.join(source_dir, \"Movies_and_TV.json\"), 'r') as fp:\n",
    "    for line in fp:\n",
    "        cnt += 1\n",
    "        ele = json.loads(line.strip())\n",
    "        obj1.append(ele)\n",
    "        if cnt >= 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overall': 5.0, 'vote': '3', 'verified': True, 'reviewTime': '02 18, 2013', 'reviewerID': 'A2VHSG6TZHU1OB', 'asin': '0001527665', 'style': {'Format:': ' Amazon Video'}, 'reviewerName': 'Ken P', 'reviewText': 'Having lived in West New Guinea (Papua) during the time period covered in this video, it is realistic, accurate, and conveys well the entrance of light and truth into a culture that was for centuries dead to and alienated from God.', 'summary': 'Realistic and Accurate', 'unixReviewTime': 1361145600}\n"
     ]
    }
   ],
   "source": [
    "print(obj1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of movies: 203766\n"
     ]
    }
   ],
   "source": [
    "# Movie data\n",
    "\n",
    "movie_data = []\n",
    "movie_detail = {}\n",
    "movie_fields = [\"Movie ID\", \"Movie title\", \"Movie category\"]\n",
    "with open(os.path.join(source_dir, \"meta_Movies_and_TV.json\"), 'r') as fp:\n",
    "    for line in fp:\n",
    "        ele = json.loads(line.strip())\n",
    "        movie_id = ele[\"asin\"].strip()\n",
    "        movie_title = ele[\"title\"].strip()\n",
    "        movie_genre = ele[\"category\"][2] if len(ele[\"category\"]) == 3 else \"unknown\"\n",
    "        movie_data.append([movie_id, movie_title, movie_genre])\n",
    "        movie_detail[movie_id] = [movie_title, movie_genre]\n",
    "\n",
    "df_movie = pd.DataFrame(movie_data, columns=movie_fields)\n",
    "print(f\"Total number of movies: {len(df_movie)}\")\n",
    "\n",
    "json.dump(movie_detail, open(os.path.join(target_dir, \"movie_detail.json\"), \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict = {}\n",
    "movie_list = list(df_movie[\"Movie ID\"])\n",
    "for id in movie_list:\n",
    "    movie_dict[id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ratings: 8752845\n"
     ]
    }
   ],
   "source": [
    "# Rating data\n",
    "from datetime import datetime\n",
    "\n",
    "rating_data = []\n",
    "rating_fields = [\"User ID\", \"Movie ID\", \"rating\", \"timestamp\", \"labels\"]\n",
    "with open(os.path.join(source_dir, \"Movies_and_TV.json\"), 'r') as fp:\n",
    "    for line in fp:\n",
    "        ele = json.loads(line.strip())\n",
    "        user_id = ele[\"reviewerID\"].strip()\n",
    "        movie_id = ele[\"asin\"].strip()\n",
    "        rating = int(ele[\"overall\"])\n",
    "        timestamp = int(datetime.strptime(ele[\"reviewTime\"], \"%m %d, %Y\").timestamp())\n",
    "        label = 1 if rating > 3 else 0\n",
    "        if movie_id in movie_dict:\n",
    "            rating_data.append([user_id, movie_id, rating, timestamp, label])\n",
    "\n",
    "df_ratings = pd.DataFrame(rating_data, columns=rating_fields)\n",
    "print(f\"Total number of ratings: {len(df_ratings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>User ID</th>\n",
       "      <th>Movie ID</th>\n",
       "      <th>Movie title</th>\n",
       "      <th>Movie category</th>\n",
       "      <th>rating</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>871920000</td>\n",
       "      <td>A1HC72VDRLANIW</td>\n",
       "      <td>6303935419</td>\n",
       "      <td>Reefer Madness VHS</td>\n",
       "      <td>Camp</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>877536000</td>\n",
       "      <td>A12LHIUPPAJ803</td>\n",
       "      <td>B000VS20M2</td>\n",
       "      <td>Blade Runner: The Final Cut</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>877536000</td>\n",
       "      <td>A12LHIUPPAJ803</td>\n",
       "      <td>B000VS20M2</td>\n",
       "      <td>Blade Runner: The Final Cut</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>877795200</td>\n",
       "      <td>A1W3KLP4PJR8EX</td>\n",
       "      <td>6303908306</td>\n",
       "      <td>The Usual Suspects, Letterbox Edition VHS</td>\n",
       "      <td>Action &amp; Adventure</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>877795200</td>\n",
       "      <td>A1W3KLP4PJR8EX</td>\n",
       "      <td>6304198493</td>\n",
       "      <td>Usual Suspects/Director's Gift Pack VHS</td>\n",
       "      <td>Action &amp; Adventure</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp         User ID    Movie ID  \\\n",
       "0  871920000  A1HC72VDRLANIW  6303935419   \n",
       "1  877536000  A12LHIUPPAJ803  B000VS20M2   \n",
       "2  877536000  A12LHIUPPAJ803  B000VS20M2   \n",
       "3  877795200  A1W3KLP4PJR8EX  6303908306   \n",
       "4  877795200  A1W3KLP4PJR8EX  6304198493   \n",
       "\n",
       "                                 Movie title      Movie category  rating  \\\n",
       "0                         Reefer Madness VHS                Camp       1   \n",
       "1                Blade Runner: The Final Cut             Fantasy       4   \n",
       "2                Blade Runner: The Final Cut             Fantasy       4   \n",
       "3  The Usual Suspects, Letterbox Edition VHS  Action & Adventure       5   \n",
       "4    Usual Suspects/Director's Gift Pack VHS  Action & Adventure       5   \n",
       "\n",
       "   labels  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge df_user/df_movie/df_rating into df_data\n",
    "\n",
    "df_data = pd.merge(df_ratings, df_movie, on=[\"Movie ID\"], how=\"inner\")\n",
    "\n",
    "df_data = df_data[df_data[\"Movie category\"] != \"unknown\"]\n",
    "\n",
    "df_data.sort_values(by=[\"timestamp\", \"User ID\", \"Movie ID\"], inplace=True, kind=\"stable\")\n",
    "\n",
    "field_names = [\"timestamp\", \"User ID\", \"Movie ID\", \"Movie title\", \"Movie category\", \"rating\", \"labels\"]\n",
    "\n",
    "df_data = df_data[field_names].reset_index(drop=True)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312226"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_10_core(data, user_col, item_col):\n",
    "    \"\"\"\n",
    "    Iteratively filters the dataset to ensure every user and item has at least 10 interactions.\n",
    "    \n",
    "    :param data: The raw dataset as a Pandas DataFrame.\n",
    "    :param user_col: Column name for users.\n",
    "    :param item_col: Column name for items.\n",
    "    :return: Filtered DataFrame where each user and item has at least 10 interactions.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "\n",
    "        # Filter users with at least 10 history interactions but no more than 200\n",
    "        user_counts = data[user_col].value_counts()\n",
    "        valid_users = user_counts[(user_counts > 10)&(user_counts <= 200)].index\n",
    "        data = data[data[user_col].isin(valid_users)]\n",
    "\n",
    "        # Filter items with at least 10 interactions\n",
    "        item_counts = data[item_col].value_counts()\n",
    "        valid_items = item_counts[(item_counts >= 10)&(item_counts <= 200)].index\n",
    "        data = data[data[item_col].isin(valid_items)]\n",
    "        \n",
    "        # Check if the dataset is stable (no more filtering needed)\n",
    "        if len(valid_users) == len(user_counts) and len(valid_items) == len(item_counts):\n",
    "            break\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a dataset `df` with columns 'user_id' and 'movie_id'\n",
    "filtered_df = filter_10_core(df_data, user_col='User ID', item_col='Movie ID')\n",
    "\n",
    "# Save the filtered dataset\n",
    "filtered_df.to_csv(os.path.join(source_dir, \"amazon_movies_10_core.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_select = {}\n",
    "movie_list = list(filtered_df[\"Movie ID\"])\n",
    "item_counts = filtered_df[\"Movie ID\"].value_counts()\n",
    "movie_list_subset = item_counts.index[:5000]\n",
    "for id in movie_list_subset:\n",
    "    movie_select[id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_filtered = filtered_df[filtered_df[\"Movie ID\"].isin(movie_select)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217004"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141829"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "217004it [00:09, 22274.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Collect user history (<= 30)\n",
    "\n",
    "user_history_dict = {\n",
    "    \"ID\": {k: [] for k in set(df_data[\"User ID\"])},\n",
    "    \"rating\": {k: [] for k in set(df_data[\"User ID\"])},\n",
    "}\n",
    "history_column = {\n",
    "    \"ID\": [],\n",
    "    \"rating\": [],\n",
    "}\n",
    "movie_id_to_title = {}\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    user_id, movie_id, rating, title = row[\"User ID\"], row[\"Movie ID\"], row[\"rating\"], row[\"Movie title\"]\n",
    "    history_column[\"ID\"].append(user_history_dict[\"ID\"][user_id].copy())\n",
    "    history_column[\"rating\"].append(user_history_dict[\"rating\"][user_id].copy())\n",
    "    user_history_dict[\"ID\"][user_id].append(movie_id)\n",
    "    user_history_dict[\"rating\"][user_id].append(rating)\n",
    "    if movie_id not in movie_id_to_title:\n",
    "        movie_id_to_title[movie_id] = title\n",
    "\n",
    "json.dump(movie_id_to_title, open(os.path.join(target_dir, \"id_to_title.json\"), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141829it [00:05, 24588.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   timestamp         User ID    Movie ID  \\\n",
      "0  930585600  A17D77DFID0GZG  6303114946   \n",
      "1  934041600   A8M4WE1MT4R4O  6304578482   \n",
      "2  934732800   A8M4WE1MT4R4O  630410605X   \n",
      "3  934732800   A8M4WE1MT4R4O  6304259093   \n",
      "4  934732800   A8M4WE1MT4R4O  6305470464   \n",
      "\n",
      "                                  Movie title      Movie category  rating  \\\n",
      "0             Searching for Bobby Fischer VHS               Drama       5   \n",
      "1  Romy and Michele's High School Reunion VHS              Comedy       3   \n",
      "2                            White Squall VHS  Action & Adventure       1   \n",
      "3                The Crow: City of Angels VHS  Action & Adventure       1   \n",
      "4                        Playing By Heart VHS              Comedy       3   \n",
      "\n",
      "   labels                                         history ID  \\\n",
      "0       1  [6300214575, 6305240655, 6300216748, 078322685...   \n",
      "1       0  [6303855555, 6305300550, 6305210144, 630245018...   \n",
      "2       0  [6303855555, 6305300550, 6305210144, 630245018...   \n",
      "3       0  [6303855555, 6305300550, 6305210144, 630245018...   \n",
      "4       0  [6303855555, 6305300550, 6305210144, 630245018...   \n",
      "\n",
      "             history rating  \n",
      "0           [5, 4, 5, 5, 4]  \n",
      "1           [4, 1, 1, 5, 5]  \n",
      "2        [4, 1, 1, 5, 5, 3]  \n",
      "3     [4, 1, 1, 5, 5, 3, 1]  \n",
      "4  [4, 1, 1, 5, 5, 3, 1, 1]  \n",
      "Number of data sampels: 141829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop data sample with history length that is less than 5.\n",
    "\n",
    "df_data[\"history ID\"] = history_column[\"ID\"]\n",
    "df_data[\"history rating\"] = history_column[\"rating\"]\n",
    "\n",
    "df_data = df_data[df_data[\"history ID\"].apply(lambda x: len(x)) >= 5].reset_index(drop=True)\n",
    "\n",
    "history_column[\"ID\"] = [x for x in history_column[\"ID\"] if len(x) >= 5]\n",
    "history_column[\"rating\"] = [x for x in history_column[\"rating\"] if len(x) >= 5]\n",
    "history_column[\"hist length\"] = [len(x) for x in history_column[\"rating\"]]\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    assert row[\"history ID\"] == history_column[\"ID\"][idx]\n",
    "    assert row[\"history rating\"] == history_column[\"rating\"][idx]\n",
    "    assert len(row[\"history rating\"]) == history_column[\"hist length\"][idx]\n",
    "\n",
    "\n",
    "print(df_data.head())\n",
    "\n",
    "print(f\"Number of data sampels: {len(df_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141829it [00:06, 23354.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID 14386\n",
      "Movie ID 5000\n",
      "Movie title 4958\n",
      "Movie category 109\n",
      "---------------------------------------------------------------\n",
      "User ID 14386 0\n",
      "Movie ID 5000 14386\n",
      "Movie title 4958 19386\n",
      "Movie category 109 24344\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode the feature dict for CTR data\n",
    "\n",
    "def add_to_dict(dict, feature):\n",
    "    if feature not in dict:\n",
    "        dict[feature] = len(dict)\n",
    "\n",
    "field_names = [\"User ID\", \"Movie ID\", \"Movie title\", \"Movie category\"]\n",
    "feature_dict = {field : {} for field in field_names}\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    for field in field_names:\n",
    "        add_to_dict(feature_dict[field], row[field])\n",
    "\n",
    "feature_count = [len(feature_dict[field]) for field in field_names]\n",
    "\n",
    "feature_offset = [0]\n",
    "for c in feature_count[:-1]:\n",
    "    feature_offset.append(feature_offset[-1] + c)\n",
    "\n",
    "for field in field_names:\n",
    "    print(field, len(feature_dict[field]))\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for f, fc, fo in zip(field_names, feature_count, feature_offset):\n",
    "    print(f, fc, fo)\n",
    "print(\"---------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the meta data for CTR\n",
    "\n",
    "meta_data = {\n",
    "    \"field_names\": field_names,\n",
    "    \"feature_count\": feature_count,\n",
    "    \"feature_dict\": feature_dict,\n",
    "    \"feature_offset\": feature_offset,\n",
    "    \"movie_id_to_title\": movie_id_to_title,\n",
    "    \"num_ratings\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "json.dump(meta_data, open(os.path.join(target_dir, \"ctr-meta.json\"), \"w\"), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict = json.load(open(os.path.join(target_dir, 'movie_detail.json')))\n",
    "meta_data = json.load(open(os.path.join(target_dir, 'ctr-meta.json')))\n",
    "id2idx = meta_data['feature_dict']['Movie ID']\n",
    "idx2movie = {idx: [movie_id] + movie_dict[movie_id] for movie_id, idx in id2idx.items()}\n",
    "json.dump(idx2movie, open(os.path.join(target_dir, 'idx2movie.json'), \"w\"), indent=4)\n",
    "json.dump(id2idx, open(os.path.join(target_dir, 'id2idx.json'), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split & save user history sequence\n",
    "\n",
    "train_num = int(0.8 * len(df_data))\n",
    "valid_num = int(0.1 * len(df_data))\n",
    "test_num = len(df_data) - train_num - valid_num\n",
    "\n",
    "history_column[\"ID\"] = [[id2idx[x] for x in hist] for hist in df_data['history ID'].tolist()]\n",
    "history_column[\"rating\"] = df_data['history rating'].tolist()\n",
    "history_column[\"hist length\"] = [len(x) for x in history_column[\"rating\"]]\n",
    "\n",
    "user_seq = {\n",
    "    \"history ID\": {\n",
    "        \"train\": history_column[\"ID\"][:train_num],\n",
    "        \"valid\": history_column[\"ID\"][train_num:train_num + valid_num],\n",
    "        \"test\": history_column[\"ID\"][train_num + valid_num:],\n",
    "    },\n",
    "    \"history rating\": {\n",
    "        \"train\": history_column[\"rating\"][:train_num],\n",
    "        \"valid\": history_column[\"rating\"][train_num:train_num + valid_num],\n",
    "        \"test\": history_column[\"rating\"][train_num + valid_num:],\n",
    "    },\n",
    "    \"history length\": {\n",
    "        \"train\": history_column[\"hist length\"][:train_num],\n",
    "        \"valid\": history_column[\"hist length\"][train_num:train_num + valid_num],\n",
    "        \"test\": history_column[\"hist length\"][train_num + valid_num:],\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field_rec = [\"User ID\", \"Movie ID\", \"rating\"]\n",
    "# df_rec = df_data[field_rec]\n",
    "# df_train_rec = df_rec[:train_num].sort_values(by=[\"User ID\", \"Movie ID\"], inplace=False, kind=\"stable\")\n",
    "# df_test_rec = df_rec[train_num + valid_num:].sort_values(by=[\"User ID\", \"Movie ID\"], inplace=False, kind=\"stable\")\n",
    "# df_train_rec.to_csv(os.path.join(target_dir, \"train.txt\"), sep=' ', index=False, header=None)\n",
    "# df_test_rec.to_csv(os.path.join(target_dir, \"test.txt\"), sep=' ', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train num: 113463\n",
      "Valid num: 14182\n",
      "Test num: 14184\n"
     ]
    }
   ],
   "source": [
    "# Save train/valid/test in parquet format\n",
    "\n",
    "df_train = df_data[:train_num].reset_index(drop=True)\n",
    "df_valid = df_data[train_num:train_num + valid_num].reset_index(drop=True)\n",
    "df_test = df_data[train_num + valid_num:].reset_index(drop=True)\n",
    "\n",
    "assert len(df_train) == train_num\n",
    "assert len(df_valid) == valid_num\n",
    "assert len(df_test) == test_num\n",
    "\n",
    "print(f\"Train num: {len(df_train)}\")\n",
    "print(f\"Valid num: {len(df_valid)}\")\n",
    "print(f\"Test num: {len(df_test)}\")\n",
    "\n",
    "df_train.to_parquet(os.path.join(target_dir, \"train.parquet.gz\"), compression=\"gzip\")\n",
    "df_valid.to_parquet(os.path.join(target_dir, \"valid.parquet.gz\"), compression=\"gzip\")\n",
    "df_test.to_parquet(os.path.join(target_dir, \"test.parquet.gz\"), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-read for sanity check\n",
    "\n",
    "train_dataset = pd.read_parquet(os.path.join(target_dir, \"train.parquet.gz\"))\n",
    "valid_dataset = pd.read_parquet(os.path.join(target_dir, \"valid.parquet.gz\"))\n",
    "test_dataset = pd.read_parquet(os.path.join(target_dir, \"test.parquet.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141829it [00:06, 20458.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctr_X (141829, 4)\n",
      "ctr_Y (141829,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert df_data to CTR data via feature_dict\n",
    "\n",
    "ctr_X, ctr_Y = [], []\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    ctr_X.append([feature_dict[field][row[field]] for field in field_names])\n",
    "    ctr_Y.append(int(row[\"labels\"]))\n",
    "\n",
    "ctr_X = np.array(ctr_X)\n",
    "ctr_Y = np.array(ctr_Y)\n",
    "print(\"ctr_X\", ctr_X.shape)\n",
    "print(\"ctr_Y\", ctr_Y.shape)\n",
    "feature_count_np = np.array(feature_count).reshape(1, -1)\n",
    "assert (ctr_X - feature_count_np <= 0).sum() == ctr_X.shape[0] * ctr_X.shape[1]\n",
    "assert (ctr_Y == 0).sum() + (ctr_Y == 1).sum() == ctr_Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the user sequence up to 30, i.e., 5 <= length <= 30.\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "user_seq_trunc = {\n",
    "    \"history ID\": {}, \n",
    "    \"history rating\": {}, \n",
    "    \"history mask\": {}, \n",
    "}\n",
    "\n",
    "for hist_name in user_seq:\n",
    "    for split in user_seq[hist_name]:\n",
    "        if hist_name != \"history length\":\n",
    "            user_seq_trunc[hist_name][split] = pad_sequence(\n",
    "                [torch.tensor(x[-30:]) for x in user_seq[hist_name][split]], \n",
    "                batch_first=True, \n",
    "            )\n",
    "        else:\n",
    "            user_seq_trunc[\"history mask\"][split] = pad_sequence(\n",
    "                [torch.ones(min(x, 30)) for x in user_seq[hist_name][split]], \n",
    "                batch_first=True, \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (113463, 4)\n",
      "valid data: (14182, 4)\n",
      "test data: (14184, 4)\n",
      "train label: (113463,)\n",
      "valid label: (14182,)\n",
      "test label: (14184,)\n"
     ]
    }
   ],
   "source": [
    "# Save CTR data & truncated user sequence into one .h5 file\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f\"ctr.h5\"), \"w\") as hf:\n",
    "    hf.create_dataset(\"train data\", data=ctr_X[:train_num, :])\n",
    "    hf.create_dataset(\"valid data\", data=ctr_X[train_num:train_num + valid_num, :])\n",
    "    hf.create_dataset(\"test data\", data=ctr_X[train_num + valid_num:, :])\n",
    "    hf.create_dataset(\"train label\", data=ctr_Y[:train_num])\n",
    "    hf.create_dataset(\"valid label\", data=ctr_Y[train_num:train_num + valid_num])\n",
    "    hf.create_dataset(\"test label\", data=ctr_Y[train_num + valid_num:])\n",
    "    for hist_name in user_seq_trunc:\n",
    "        for split in user_seq_trunc[hist_name]:\n",
    "            hf.create_dataset(f\"{split} {hist_name}\", data=user_seq_trunc[hist_name][split])\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f\"ctr.h5\"), \"r\") as hf:\n",
    "    assert (ctr_X - np.concatenate([hf[\"train data\"][:], hf[\"valid data\"][:], hf[\"test data\"][:]], axis=0)).sum() == 0\n",
    "    assert (ctr_Y - np.concatenate([hf[\"train label\"][:], hf[\"valid label\"][:], hf[\"test label\"][:]], axis=0)).sum() == 0\n",
    "    for hist_name in user_seq_trunc:\n",
    "        for split in user_seq_trunc[hist_name]:\n",
    "            assert (user_seq_trunc[hist_name][split] - hf[f\"{split} {hist_name}\"][:]).sum() == 0\n",
    "\n",
    "    x = hf[\"train data\"][:]\n",
    "    assert (x - ctr_X[:train_num, :]).sum() == 0\n",
    "    print(f\"train data: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"valid data\"][:]\n",
    "    assert (x - ctr_X[train_num:train_num + valid_num, :]).sum() == 0\n",
    "    print(f\"valid data: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"test data\"][:]\n",
    "    assert (x - ctr_X[train_num + valid_num:, :]).sum() == 0\n",
    "    print(f\"test data: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"train label\"][:]\n",
    "    assert (x - ctr_Y[:train_num]).sum() == 0\n",
    "    print(f\"train label: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"valid label\"][:]\n",
    "    assert (x - ctr_Y[train_num:train_num + valid_num]).sum() == 0\n",
    "    print(f\"valid label: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"test label\"][:]\n",
    "    assert (x - ctr_Y[train_num + valid_num:]).sum() == 0\n",
    "    print(f\"test label: {x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_counts = df_data['Movie ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['B0000TG8UM', 'B0015OKWL2', 'B0015OKWKS', 'B00005JLX2', 'B00008972L',\n",
      "       'B0000AGQ2H', 'B00013RC2K', 'B0012DUT3M', 'B0000797IO', 'B00028G748',\n",
      "       ...\n",
      "       '6304057679', '6301880447', 'B000059QY3', 'B0000AGQ2M', 'B000059WIW',\n",
      "       'B00008DDJD', 'B00016MCAE', 'B00005JHCF', 'B0000C0FC0', 'B000WW1YOM'],\n",
      "      dtype='object', name='Movie ID', length=5000)\n"
     ]
    }
   ],
   "source": [
    "# train_set['Movie ID'].value_counts()\n",
    "multi_occurrences = movie_counts[movie_counts > 1].index\n",
    "print(multi_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (133088, 3)\n",
      "Test set shape: (8741, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove the timestamps\n",
    "field_rec = [\"User ID\", \"Movie ID\", \"rating\"]\n",
    "df_ratings = df_data[field_rec]\n",
    "\n",
    "# Group by each user and filter for users with more than 10 ratings\n",
    "user_groups = df_ratings.groupby('User ID').filter(lambda x: len(x) > 5)\n",
    "\n",
    "# Extract the last record for each user in the test set and the rest as training\n",
    "tail_set = user_groups.groupby('User ID').tail(1)\n",
    "test_set = tail_set[tail_set['Movie ID'].isin(multi_occurrences)]\n",
    "train_set = df_ratings.drop(test_set.index)\n",
    "\n",
    "# Display the shapes to confirm the split\n",
    "print(\"Training set shape:\", train_set.shape)\n",
    "print(\"Test set shape:\", test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         User ID  Movie ID  rating\n",
      "25             1        48       5\n",
      "136            2      1917       3\n",
      "232            3      2081       4\n",
      "243            4      1954       5\n",
      "258            5       288       2\n",
      "...          ...       ...     ...\n",
      "999251      6036      2643       1\n",
      "999684      6037       435       3\n",
      "999731      6038      1183       5\n",
      "999826      6039      1254       4\n",
      "1000042     6040      1221       4\n",
      "\n",
      "[6038 rows x 3 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as .txt files\n",
    "df_train_rec = train_set.sort_values(by=[\"User ID\", \"Movie ID\"], inplace=False, kind=\"stable\")\n",
    "df_test_rec = test_set.sort_values(by=[\"User ID\", \"Movie ID\"], inplace=False, kind=\"stable\")\n",
    "df_train_rec.to_csv(os.path.join(target_dir, \"train.txt\"), sep=' ', index=False, header=None)\n",
    "df_test_rec.to_csv(os.path.join(target_dir, \"test.txt\"), sep=' ', index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cache/sichunluo2/miniconda3/envs/mt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from transformers import set_seed\n",
    "import hashlib\n",
    "import json\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "dataset_name = \"BookCrossing\"\n",
    "root = f\"../data/{dataset_name}\"\n",
    "source_dir = os.path.join(root, \"raw_data\")\n",
    "target_dir = os.path.join(root, \"proc_data\")\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters, digits\n",
    "\n",
    "def character_check(item, special_letters=\"\"):\n",
    "    for letter in str(item):\n",
    "        if letter not in ascii_letters + digits + special_letters:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def isin_selected(item, selected_dict):\n",
    "    if item in selected_dict:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_users 111bda80ee793f1efcaf0f58cb920771\n"
     ]
    }
   ],
   "source": [
    "# Read user info\n",
    "\n",
    "user_fields = [\"User ID\", \"Location\", \"Age\"]\n",
    "pattern = re.compile(r'NULL|\".*?(?<!\\\\)\"', re.S)\n",
    "with open(os.path.join(source_dir, \"BX-Users.csv\"), 'r', encoding='cp1252') as f:\n",
    "    content = pattern.findall(f.read())\n",
    "    content = [s[1:-1] if s != 'NULL' else None for s in content]\n",
    "    processed_list = list(np.array(content).reshape((-1, 3)))\n",
    "    processed_list.pop(0)\n",
    "    df_users = pd.DataFrame(processed_list, columns=user_fields)\n",
    "\n",
    "# There are messy info/code (or totally empty) in the `Location` field, we only use the country instead.\n",
    "# E.g., ['&#37073;&#24030;&#26159;, &#20013;&#22269;&#27827;&#21335;&#30465;&#37073;&#24030;&#24066;, china', \n",
    "#        'philippine science high school - cmc, mcc main stadium, sagadan, tubod, lanao del norte, philippines', \n",
    "#        '6.a.4.a.6.a`4.a, 6.a.4.a.6.a`4.a.6.a`4.a.6.a.4.a.6.a`4.aoe6.a`4.a -- 6.a.4.a.6.a`4.aoe6.a`4.a ã??, ä¸\\xadå?½']\n",
    "def convert_location_to_country(x):\n",
    "    x = x.split(', ')[-1].strip().title().replace(\"!\", \"\").strip()\n",
    "    if x.lower() in [\"usa\", \"us\", \"u s\", \"u s a\"]:\n",
    "        x = \"USA\"\n",
    "    if x.lower() in [\"uk\", \"u k\"]:\n",
    "        x = \"UK\"\n",
    "    while len(x) > 0 and x[-1] in [\",\", \".\"]:\n",
    "        x = x[:-1]\n",
    "    while len(x) > 0 and x[0] in [\",\", \".\"]:\n",
    "        x = x[1:]\n",
    "    if \"U.S\" in x.upper() and x != \"U.S. Virgin Islands\":\n",
    "        x = \"USA\"\n",
    "    if x in [\"San José\", \"San Josï¿½\"]:\n",
    "        x = \"USA\"\n",
    "    if x in [\"España\", \"Castilla-León\", \"Espaã±A\", \"Cataluña\", \"Mérida\", \"Álava\", \"Málaga\", \"A Coruña\", \"Barcelonès\", \"Berguedà\",\n",
    "              \"Espaï¿½A\", \"Castilla-Leï¿½N\", \"A Coruï¿½A\", \"Cataluï¿½A\", \"Barcelonï¿½S\", \"Ï¿½Lava\", \"Mï¿½Rida\", \"Berguedï¿½\", \"Mï¿½Laga\"] or \"spain\" in x.lower():\n",
    "        x = \"Spain\"\n",
    "    if x in [\"L`Italia\"]:\n",
    "        x = \"Italy\"\n",
    "    if x in [\"Baden-Württemberg\", \"Bademn Würtemberg\", \"Baden-Wï¿½Rttemberg\", \"Bademn Wï¿½Rtemberg\"]:\n",
    "        x = \"German\"\n",
    "    if x in [\"Cote D`Ivoire\", \"Côte D\", \"Cï¿½Te D\"]:\n",
    "        x = \"Ivory Coast\"\n",
    "    if x in [\"Oberösterreich\", \"Oberï¿½Sterreich\"]:\n",
    "        x = \"Austria\"\n",
    "    if x in [\"México\", \"Mï¿½Xico\"]:\n",
    "        x = \"Mexico\"\n",
    "    if x in [\"Türkiye\", \"Içel\", \"Tï¿½Rkiye\"]:\n",
    "        x = \"Turkey\"\n",
    "    if x in [\"L`Algérie\", \"Algérie\", \"Kärnten\", \"Kï¿½Rnten\", \"L`Algï¿½Rie\", \"Algï¿½Rie\"]:\n",
    "        x = \"Algeria\"\n",
    "    if \"Brasil\" in x:\n",
    "        x = \"Brazil\"\n",
    "    if x in [\"Rhône-Alpes\", \"Rhône Alpes\", \"Rhï¿½Ne-Alpes\", \"Rhï¿½Ne Alpes\"]:\n",
    "        x = \"France\"\n",
    "    if \"Greece\" in x:\n",
    "        x = \"Greece\"\n",
    "    if x in [\"Santarém\", \"Santarï¿½M\"]:\n",
    "        x = \"Portugal\"\n",
    "    if x in [\"Länsi-Suomen Lääni\", \"Lï¿½Nsi-Suomen Lï¿½Ï¿½Ni\"]:\n",
    "        x = \"Finland\"\n",
    "    if x in [\"V.Götaland\", \"Nyhamnsläge\", \"V.Gï¿½Taland\", \"Nyhamnslï¿½Ge\"]:\n",
    "        x = \"Sweden\"\n",
    "    if x in [\"Moçambique\", \"Moï¿½Ambique\"]:\n",
    "        x = \"Mozambique\"\n",
    "    if x in [\"Ix Región\", \"Ix Regiï¿½N\"]:\n",
    "        x = \"Chile\"\n",
    "    if x in [\"Maï¿½Opolskie\", \"Ma³Opolskie\"]:\n",
    "        x = \"Poland\"\n",
    "    if x in [\"Perï¿½\", \"Perãº\"]:\n",
    "        x = \"Peru\"\n",
    "    if x != \"China\" and (\"china\" in x.lower() or x == \"La Chine Éternelle\" or x == \"La Chine Ï¿½Ternelle\"):\n",
    "        x = \"China\"\n",
    "    if x == \"Ï¿½Ï¿½Ï¿½\":\n",
    "        x = \"China\"\n",
    "    if (x == \"\" or \\\n",
    "        x in [\"Öð¹Ú\", \"ºþäï\", \"We`Re Global\", \"Ï¿½Ï¿½Ï¿½Ï¿½\", \"Iï¿½El\"] or \\\n",
    "        len(x) == 1 or \\\n",
    "        \"N/A\" in x or \\\n",
    "        \"&#\" in x or \\\n",
    "        \"?\" in x or \\\n",
    "        \"@\" in x or \\\n",
    "        \"*\" in x):\n",
    "        x = \"unknown\"\n",
    "    return x\n",
    "df_users[\"Location\"] = df_users[\"Location\"].apply(convert_location_to_country)\n",
    "df_users[\"location_check\"] = df_users[\"Location\"].apply(lambda x: character_check(x, special_letters=\"- .&/()\"))\n",
    "\n",
    "assert len(df_users.loc[df_users[\"location_check\"] == 1, \"Location\"]) == 0\n",
    "\n",
    "# Nearly a half of the features in `Age` field are missing.\n",
    "def convert_age_to_bucket(x):\n",
    "    if x is None:\n",
    "        x = \"unknown\"\n",
    "    else:\n",
    "        x = int(x)\n",
    "        # There are out-of-range ages (e.g., < 5 or > 100).\n",
    "        if x < 5 or x > 100:\n",
    "            x = \"unknown\"\n",
    "        # Age discretization\n",
    "        elif x < 18:\n",
    "            x = \"under 18\"\n",
    "        elif 18 <= x < 25:\n",
    "            x = \"18-24\"\n",
    "        elif 25 <= x < 30:\n",
    "            x = \"25-29\"\n",
    "        elif 30 <= x < 35:\n",
    "            x = \"30-34\"\n",
    "        elif 35 <= x < 40:\n",
    "            x = \"35-39\"\n",
    "        elif 40 <= x < 45:\n",
    "            x = \"40-44\"\n",
    "        elif 45 <= x < 50:\n",
    "            x = \"45-49\"\n",
    "        elif 50 <= x < 55:\n",
    "            x = \"50-54\"\n",
    "        elif 55 <= x < 60:\n",
    "            x = \"55-59\"\n",
    "        else:\n",
    "            x = \"60+\"\n",
    "    return x\n",
    "df_users[\"Age\"] = df_users[\"Age\"].apply(convert_age_to_bucket)\n",
    "\n",
    "for field in user_fields:\n",
    "    for s in list(df_users[field]):\n",
    "        if field == \"User ID\":\n",
    "            assert 1 <= int(s) <= 278858\n",
    "        if field == \"Location\":\n",
    "            assert 2 <= len(s) <= 45\n",
    "        if field == \"Age\":\n",
    "            assert s in [\"unknown\", \"under 18\" ,\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60+\"]\n",
    "\n",
    "df_users = df_users[user_fields]\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(df_users.values.tolist(), sort_keys=True).encode('utf-8')).hexdigest()\n",
    "print(\"df_users\", md5_hash)\n",
    "assert md5_hash == \"111bda80ee793f1efcaf0f58cb920771\"\n",
    "# df_users.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users['check'] = df_users['User ID'].apply(lambda x: isin_selected(x, user_selected))\n",
    "df_users = df_users[df_users['check'] == 1]\n",
    "df_users = df_users[user_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8723</td>\n",
       "      <td>8723</td>\n",
       "      <td>8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8723</td>\n",
       "      <td>109</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>278843</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>6632</td>\n",
       "      <td>2683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User ID Location      Age\n",
       "count     8723     8723     8723\n",
       "unique    8723      109       11\n",
       "top     278843      USA  unknown\n",
       "freq         1     6632     2683"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ISBN                                         Book title  \\\n",
      "0  0195153448                                Classical Mythology   \n",
      "1  0002005018                                       Clara Callan   \n",
      "2  0060973129                               Decision in Normandy   \n",
      "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
      "4  0393045218                             The Mummies of Urumchi   \n",
      "\n",
      "                 Author Publication year                Publisher  \n",
      "0    Mark P. O. Morford             2002  Oxford University Press  \n",
      "1  Richard Bruce Wright             2001    HarperFlamingo Canada  \n",
      "2          Carlo D'Este             1991          HarperPerennial  \n",
      "3      Gina Bari Kolata             1999     Farrar Straus Giroux  \n",
      "4       E. J. W. Barber             1999   W. W. Norton & Company  \n",
      "---------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 271376 entries, 0 to 271378\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   ISBN              271376 non-null  object\n",
      " 1   Book title        271376 non-null  object\n",
      " 2   Author            271376 non-null  object\n",
      " 3   Publication year  271376 non-null  object\n",
      " 4   Publisher         271376 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 12.4+ MB\n",
      "None\n",
      "---------------------------------------------------------------\n",
      "              ISBN      Book title           Author Publication year  \\\n",
      "count       271376          271376           271376           271376   \n",
      "unique      271375          242152           102027              116   \n",
      "top     0486404242  Selected Poems  Agatha Christie             2002   \n",
      "freq             2              27              632            17627   \n",
      "\n",
      "        Publisher  \n",
      "count      271376  \n",
      "unique      16807  \n",
      "top     Harlequin  \n",
      "freq         7536  \n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Read book info\n",
    "\n",
    "book_fields = [\"ISBN\", \"Book title\", \"Author\", \"Publication year\", \"Publisher\"]\n",
    "pattern = re.compile(r'(?<=\");(?=\")')\n",
    "processed_list = []\n",
    "with open(os.path.join(source_dir, \"BX-Books.csv\"), 'r', encoding='cp1252') as f:\n",
    "    for line in f.readlines():\n",
    "        split_line = pattern.split(line.strip())\n",
    "        split_line = [item[1:-1].strip('\\t') for item in split_line][:-3] # The last three image URLs are not needed.\n",
    "        processed_list.append(split_line)\n",
    "    processed_list.pop(0)\n",
    "    df_books = pd.DataFrame(processed_list, columns=book_fields)\n",
    "\n",
    "# ISBN should only contain letters and digits.\n",
    "df_books['ISBN_check'] = df_books['ISBN'].apply(lambda x: character_check(x))\n",
    "df_books = df_books[df_books['ISBN_check'] == 0]\n",
    "\n",
    "# There are invalid publication years, i.e., \"0\"\n",
    "def convert_publication_year(x):\n",
    "    x = x if len(x) == 4 else \"unknown\"\n",
    "    return x\n",
    "df_books[\"Publication year\"] = df_books[\"Publication year\"].apply(convert_publication_year)\n",
    "\n",
    "df_books[\"Publisher\"] = df_books[\"Publisher\"].apply(lambda x: x if x.lower() != \"n/a\" else \"unknown\")\n",
    "df_books[\"Author\"] = df_books[\"Author\"].apply(lambda x: x if x.lower() != \"n/a\" else \"unknown\")\n",
    "\n",
    "for field in book_fields:\n",
    "    for s in list(df_books[field]):\n",
    "        if field == \"ISBN\":\n",
    "            assert len(s) == 10\n",
    "        if field == \"Book title\":\n",
    "            assert 1 <= len(s) <= 256\n",
    "        if field == \"Author\":\n",
    "            assert 1 <= len(s) <= 143\n",
    "        if field == \"Publication year\":\n",
    "            assert s == \"unknown\" or len(s) == 4\n",
    "        if field == \"Publisher\":\n",
    "            assert 1 <= len(s) <= 134\n",
    "\n",
    "df_books = df_books[book_fields]\n",
    "print(df_books.head())\n",
    "print('---------------------------------------------------------------')\n",
    "print(df_books.info())\n",
    "print('---------------------------------------------------------------')\n",
    "print(df_books.describe())\n",
    "print('---------------------------------------------------------------')\n",
    "# md5_hash = hashlib.md5(json.dumps(df_books.values.tolist(), sort_keys=True).encode('utf-8')).hexdigest()\n",
    "# print(\"df_books\", md5_hash)\n",
    "# assert md5_hash == \"39d643d7c252ea60633e28cc3328ee82\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibsn_selected = json.load(open(os.path.join(target_dir, 'isbn2id.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter ISBN with less than 10 records.\n",
    "df_books['ISBN_check'] = df_books['ISBN'].apply(lambda x: isin_selected(x, ibsn_selected))\n",
    "df_books = df_books[df_books['ISBN_check'] == 1]\n",
    "df_books = df_books[book_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication year</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3547</td>\n",
       "      <td>3547</td>\n",
       "      <td>3547</td>\n",
       "      <td>3547</td>\n",
       "      <td>3547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3547</td>\n",
       "      <td>3164</td>\n",
       "      <td>1199</td>\n",
       "      <td>44</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>3803112133</td>\n",
       "      <td>Bridget Jones's Diary</td>\n",
       "      <td>Stephen King</td>\n",
       "      <td>2002</td>\n",
       "      <td>Berkley Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>97</td>\n",
       "      <td>399</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN             Book title        Author Publication year  \\\n",
       "count         3547                   3547          3547             3547   \n",
       "unique        3547                   3164          1199               44   \n",
       "top     3803112133  Bridget Jones's Diary  Stephen King             2002   \n",
       "freq             1                      5            97              399   \n",
       "\n",
       "                       Publisher  \n",
       "count                       3547  \n",
       "unique                       310  \n",
       "top     Berkley Publishing Group  \n",
       "freq                         197  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID 8723\n",
      "Location 109\n",
      "Age 11\n",
      "ISBN 3547\n",
      "Book title 3164\n",
      "Author 1199\n",
      "Publication year 44\n",
      "Publisher 310\n"
     ]
    }
   ],
   "source": [
    "# Encode features\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_to_dict(dict, feature):\n",
    "    if feature not in dict:\n",
    "        dict[feature] = len(dict)\n",
    "\n",
    "feature_dict = {field : {} for field in user_fields + book_fields}\n",
    "user_dict = {}\n",
    "book_dict = {}\n",
    "\n",
    "for idx, row in df_users.iterrows():\n",
    "    if row[\"User ID\"] not in user_dict:\n",
    "        user_dict[row[\"User ID\"]] = [row[\"Location\"], row[\"Age\"]]\n",
    "    for field in user_fields:\n",
    "        add_to_dict(feature_dict[field], row[field])\n",
    "\n",
    "for idx, row in df_books.iterrows():\n",
    "    if row[\"ISBN\"] not in book_dict:\n",
    "        book_dict[row[\"ISBN\"]] = [row[\"Book title\"], row[\"Author\"], row[\"Publication year\"], row[\"Publisher\"]]\n",
    "    for field in book_fields:\n",
    "        add_to_dict(feature_dict[field], row[field])\n",
    "\n",
    "feature_count = [len(feature_dict[field]) for field in user_fields + book_fields]\n",
    "\n",
    "for field in user_fields:\n",
    "    print(field, len(feature_dict[field]))\n",
    "    assert len(feature_dict[field]) == len(set(list(df_users[field])))\n",
    "\n",
    "for field in book_fields:\n",
    "    print(field, len(feature_dict[field]))\n",
    "    assert len(feature_dict[field]) == len(set(list(df_books[field])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(book_dict, open(os.path.join(target_dir, \"book_dict.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3547"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3547"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_dict[\"ISBN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"User-ID\";\"ISBN\";\"Book-Rating\"\n",
      "\n",
      "\"276725\";\"034545104X\";\"0\"\n",
      "\n",
      "\"276726\";\"0155061224\";\"5\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cnt = 0\n",
    "# with open(os.path.join(source_dir, \"BX-Book-Ratings.csv\"), 'r', encoding='cp1252') as f:\n",
    "#     for line in f.readlines():\n",
    "#         split_line = line.strip().split(';')\n",
    "#         split_line = [item[1:-1] for item in split_line]\n",
    "#         if cnt < 3:\n",
    "#             print(line)\n",
    "#         cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def filter_10_core(data, user_col, item_col):\n",
    "#     \"\"\"\n",
    "#     Iteratively filters the dataset to ensure every user and item has at least 10 interactions.\n",
    "    \n",
    "#     :param data: The raw dataset as a Pandas DataFrame.\n",
    "#     :param user_col: Column name for users.\n",
    "#     :param item_col: Column name for items.\n",
    "#     :return: Filtered DataFrame where each user and item has at least 10 interactions.\n",
    "#     \"\"\"\n",
    "#     while True:\n",
    "\n",
    "#         # Filter users with at least 10 history interactions but no more than 200\n",
    "#         user_counts = data[user_col].value_counts()\n",
    "#         valid_users = user_counts[(user_counts > 10)&(user_counts <= 200)].index\n",
    "#         data = data[data[user_col].isin(valid_users)]\n",
    "\n",
    "#         # Filter items with at least 10 interactions\n",
    "#         item_counts = data[item_col].value_counts()\n",
    "#         valid_items = item_counts[item_counts >= 10].index\n",
    "#         data = data[data[item_col].isin(valid_items)]\n",
    "        \n",
    "#         # Check if the dataset is stable (no more filtering needed)\n",
    "#         if len(valid_users) == len(user_counts) and len(valid_items) == len(item_counts):\n",
    "#             break\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming you have a dataset `df` with columns 'user_id' and 'item_id'\n",
    "# processed_list1 = []\n",
    "# with open(os.path.join(source_dir, \"BX-Book-Ratings.csv\"), 'r', encoding='cp1252') as f:\n",
    "#     for line in f.readlines():\n",
    "#         split_line = line.strip().split(';')\n",
    "#         split_line = [item[1:-1] for item in split_line]\n",
    "#         processed_list1.append(split_line)\n",
    "#     column_list = processed_list1[0]\n",
    "#     processed_list1.pop(0)\n",
    "# df1 = pd.DataFrame(processed_list1, columns=['User ID', 'ISBN', \"Rating\"])\n",
    "# filtered_df = filter_10_core(df1, user_col='User ID', item_col='ISBN')\n",
    "\n",
    "# # Save the filtered dataset\n",
    "# filtered_df.to_csv(os.path.join(source_dir, \"bookcrossing_10_core.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>89744</td>\n",
       "      <td>89744</td>\n",
       "      <td>89744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3575</td>\n",
       "      <td>3665</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>43619</td>\n",
       "      <td>0971880107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>146</td>\n",
       "      <td>531</td>\n",
       "      <td>51654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User ID        ISBN Rating\n",
       "count    89744       89744  89744\n",
       "unique    3575        3665     11\n",
       "top      43619  0971880107      0\n",
       "freq       146         531  51654"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3575"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_counts = filtered_df['User ID'].value_counts()\n",
    "# multi_occurrences = user_counts[user_counts > 10].index\n",
    "# len(multi_occurrences)\n",
    "\n",
    "# book_counts = filtered_df['ISBN'].value_counts()\n",
    "# multi_occurrences = book_counts[book_counts >= 10].index\n",
    "# len(multi_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ratings\n",
    "\n",
    "processed_list = []\n",
    "with open(os.path.join(source_dir, \"BX-Book-Ratings.csv\"), 'r', encoding='cp1252') as f:\n",
    "    for line in f.readlines():\n",
    "        split_line = line.strip().split(';')\n",
    "        split_line = [item[1:-1] for item in split_line]\n",
    "        processed_list.append(split_line)\n",
    "    column_list = processed_list[0]\n",
    "    processed_list.pop(0)\n",
    "\n",
    "user_hist, hist_rating, labels = {}, {}, {}\n",
    "for user, isbn, rating in processed_list:\n",
    "    if user in feature_dict[\"User ID\"] and isbn in ibsn_selected:\n",
    "        if user not in user_hist:\n",
    "            user_hist[user] = []\n",
    "            hist_rating[user] = []\n",
    "            labels[user] = []\n",
    "        user_hist[user].append(isbn)\n",
    "        hist_rating[user].append(int(rating))\n",
    "        labels[user].append(int(int(rating) >= 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_list = []\n",
    "# filtered_df = pd.read_csv(os.path.join(source_dir, \"bookcrossing_10_core.csv\"))\n",
    "# columns = filtered_df.columns\n",
    "# for idx, line in filtered_df.iterrows():\n",
    "#     split_line = [str(line[col]) for col in columns]\n",
    "#     processed_list.append(split_line)\n",
    "    \n",
    "# item_cnt = {}\n",
    "# for user, isbn, rating in processed_list:\n",
    "#     if isbn not in item_cnt:\n",
    "#         item_cnt[isbn] = 0\n",
    "#     item_cnt[isbn] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_hist, hist_rating, labels = {}, {}, {}\n",
    "# for user, isbn, rating in processed_list:\n",
    "#     if user in feature_dict[\"User ID\"] and isbn in feature_dict[\"ISBN\"]:\n",
    "#         if user not in user_hist:\n",
    "#             user_hist[user] = []\n",
    "#             hist_rating[user] = []\n",
    "#             labels[user] = []\n",
    "#         user_hist[user].append(isbn)\n",
    "#         hist_rating[user].append(int(rating))\n",
    "#         labels[user].append(int(int(rating) >= 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46447"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3547"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_dict[\"ISBN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users deleted: 37724\n"
     ]
    }
   ],
   "source": [
    "# filter users who rated no more than 5 books\n",
    "user_del = []\n",
    "for user, hist in user_hist.items():\n",
    "    if len(hist) < 5:\n",
    "        user_del.append(user)\n",
    "\n",
    "print(\"Number of users deleted:\", len(user_del))\n",
    "\n",
    "for user in user_del:\n",
    "    del user_hist[user]\n",
    "    del hist_rating[user]\n",
    "    del labels[user]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8723"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8723\n",
      "3547\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "book_list = set()\n",
    "for user in user_hist.keys():\n",
    "    zipped_data = list(zip(user_hist[user], hist_rating[user], labels[user]))\n",
    "    set_seed(42)\n",
    "    random.shuffle(zipped_data)\n",
    "    user_hist[user], hist_rating[user], labels[user] = map(list, zip(*zipped_data))\n",
    "    isbn = user_hist[user][-1]\n",
    "    data_sample = copy.deepcopy([user] + user_dict[user] + [isbn] + book_dict[isbn] +\n",
    "                                    [user_hist[user][:-1]] + [hist_rating[user][:-1]] + [labels[user][-1]] + [hist_rating[user][-1]])\n",
    "    data_list.append(data_sample)\n",
    "    book_list.update(user_hist[user][:])\n",
    "\n",
    "print(len(data_list))\n",
    "print(len(book_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibsn_selected = {}\n",
    "for ibsn in book_list:\n",
    "    ibsn_selected[ibsn] = 1\n",
    "\n",
    "user_selected = {}\n",
    "for user in user_hist:\n",
    "    user_selected[user] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.46222629829187206\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "for user in labels.keys():\n",
    "    values.append(labels[user][-1])\n",
    "print(max(values),sum(values)/len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8723"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg 19.68824163969795\n",
      "**************************************************\n",
      "Hist lens / Number of users\n",
      "4 1391\n",
      "5 983\n",
      "6 739\n",
      "7 574\n",
      "8 406\n",
      "9 273\n",
      "10 365\n",
      "11 292\n",
      "12 243\n",
      "13 214\n",
      "14 176\n",
      "15 176\n",
      "16 160\n",
      "17 133\n",
      "18 126\n",
      "19 98\n",
      "20 106\n",
      "21 89\n",
      "22 80\n",
      "23 88\n",
      "24 70\n",
      "25 77\n",
      "26 60\n",
      "27 57\n",
      "28 58\n",
      "29 44\n",
      "30 71\n",
      "31 43\n",
      "32 49\n",
      "33 34\n",
      "34 34\n",
      "35 34\n",
      "36 31\n",
      "37 38\n",
      "38 31\n",
      "39 54\n",
      "40 23\n",
      "41 26\n",
      "42 15\n",
      "43 24\n",
      "44 29\n",
      "45 25\n",
      "46 28\n",
      "47 31\n",
      "48 26\n",
      "49 23\n",
      "50 17\n",
      "51 17\n",
      "52 23\n",
      "53 18\n",
      "54 15\n",
      "55 20\n",
      "56 15\n",
      "57 12\n",
      "58 15\n",
      "59 17\n",
      "60 18\n",
      "61 14\n",
      "62 14\n",
      "63 12\n",
      "64 11\n",
      "65 10\n",
      "66 16\n",
      "67 17\n",
      "68 9\n",
      "69 15\n",
      "70 12\n",
      "71 14\n",
      "72 14\n",
      "73 15\n",
      "74 9\n",
      "75 14\n",
      "76 11\n",
      "77 7\n",
      "78 6\n",
      "79 7\n",
      "80 10\n",
      "81 7\n",
      "82 4\n",
      "83 10\n",
      "84 6\n",
      "85 7\n",
      "86 7\n",
      "87 7\n",
      "88 9\n",
      "89 8\n",
      "90 6\n",
      "91 9\n",
      "92 9\n",
      "93 6\n",
      "94 6\n",
      "95 8\n",
      "96 5\n",
      "97 4\n",
      "98 5\n",
      "99 9\n",
      "100 4\n",
      "101 2\n",
      "102 6\n",
      "103 8\n",
      "104 5\n",
      "105 3\n",
      "106 10\n",
      "107 8\n",
      "108 7\n",
      "109 5\n",
      "110 1\n",
      "111 4\n",
      "112 4\n",
      "113 3\n",
      "114 4\n",
      "115 3\n",
      "116 5\n",
      "117 4\n",
      "118 4\n",
      "119 3\n",
      "120 4\n",
      "121 4\n",
      "122 4\n",
      "123 3\n",
      "124 4\n",
      "125 7\n",
      "126 4\n",
      "127 2\n",
      "129 5\n",
      "130 2\n",
      "131 5\n",
      "132 4\n",
      "133 4\n",
      "134 6\n",
      "135 2\n",
      "136 3\n",
      "137 7\n",
      "139 4\n",
      "140 2\n",
      "141 3\n",
      "142 4\n",
      "143 2\n",
      "144 2\n",
      "145 4\n",
      "147 1\n",
      "148 2\n",
      "149 4\n",
      "150 1\n",
      "151 1\n",
      "152 4\n",
      "154 4\n",
      "155 4\n",
      "156 3\n",
      "157 2\n",
      "158 2\n",
      "159 1\n",
      "160 7\n",
      "161 1\n",
      "162 1\n",
      "164 1\n",
      "165 2\n",
      "166 3\n",
      "168 2\n",
      "169 1\n",
      "170 6\n",
      "171 2\n",
      "173 3\n",
      "174 2\n",
      "175 4\n",
      "176 6\n",
      "177 3\n",
      "179 2\n",
      "183 2\n",
      "185 2\n",
      "187 2\n",
      "188 3\n",
      "189 1\n",
      "192 1\n",
      "193 1\n",
      "195 4\n",
      "197 1\n",
      "198 3\n",
      "199 1\n",
      "200 2\n",
      "201 1\n",
      "202 2\n",
      "204 3\n",
      "205 3\n",
      "207 1\n",
      "208 3\n",
      "209 1\n",
      "210 2\n",
      "212 1\n",
      "214 1\n",
      "217 2\n",
      "218 1\n",
      "221 3\n",
      "222 1\n",
      "223 2\n",
      "224 1\n",
      "225 2\n",
      "227 1\n",
      "228 1\n",
      "237 2\n",
      "239 1\n",
      "240 1\n",
      "242 1\n",
      "243 1\n",
      "244 1\n",
      "246 1\n",
      "247 1\n",
      "248 2\n",
      "249 1\n",
      "250 2\n",
      "252 2\n",
      "254 1\n",
      "255 1\n",
      "258 1\n",
      "261 1\n",
      "262 1\n",
      "263 2\n",
      "264 2\n",
      "265 1\n",
      "267 3\n",
      "269 2\n",
      "274 1\n",
      "275 2\n",
      "282 2\n",
      "284 1\n",
      "286 1\n",
      "287 1\n",
      "290 1\n",
      "292 1\n",
      "299 1\n",
      "301 2\n",
      "302 1\n",
      "307 1\n",
      "309 1\n",
      "310 1\n",
      "311 1\n",
      "313 1\n",
      "315 2\n",
      "318 1\n",
      "319 1\n",
      "321 1\n",
      "323 2\n",
      "331 2\n",
      "334 2\n",
      "347 1\n",
      "349 2\n",
      "350 1\n",
      "358 1\n",
      "362 1\n",
      "368 1\n",
      "371 1\n",
      "372 1\n",
      "379 1\n",
      "385 1\n",
      "387 1\n",
      "396 1\n",
      "397 2\n",
      "400 1\n",
      "408 1\n",
      "423 1\n",
      "426 1\n",
      "430 1\n",
      "432 1\n",
      "436 1\n",
      "439 1\n",
      "445 1\n",
      "449 1\n",
      "494 1\n",
      "511 1\n",
      "513 1\n",
      "516 1\n",
      "525 1\n",
      "549 2\n",
      "551 1\n",
      "559 1\n",
      "581 1\n",
      "592 1\n",
      "597 1\n",
      "604 1\n",
      "615 1\n",
      "629 1\n",
      "731 1\n",
      "738 1\n",
      "858 1\n",
      "973 1\n",
      "2116 1\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "count = {}\n",
    "for user, hist in user_hist.items():\n",
    "    cnt += len(hist) - 1\n",
    "    if len(hist)-1 not in count:\n",
    "        count[len(hist)-1] = 0\n",
    "    count[len(hist)-1] += 1\n",
    "print(\"avg\", cnt/11124)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"Hist lens / Number of users\")\n",
    "for cnt in sorted(count.keys()):\n",
    "    print(cnt, count[cnt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 8723\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication year</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>user_hist</th>\n",
       "      <th>hist_rating</th>\n",
       "      <th>labels</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242790</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0312976275</td>\n",
       "      <td>Hot Six : A Stephanie Plum Novel (A Stephanie ...</td>\n",
       "      <td>Janet Evanovich</td>\n",
       "      <td>2001</td>\n",
       "      <td>St. Martin's Paperbacks</td>\n",
       "      <td>[0449003787, 0312980140, 0425155404, 0449003795]</td>\n",
       "      <td>[6, 10, 0, 9]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>205059</td>\n",
       "      <td>India</td>\n",
       "      <td>60+</td>\n",
       "      <td>0099244926</td>\n",
       "      <td>The Street Lawyer</td>\n",
       "      <td>John Grisham</td>\n",
       "      <td>1999</td>\n",
       "      <td>Random House Uk Ltd</td>\n",
       "      <td>[0449227421, 0440211727, 0440236673, 0449227545]</td>\n",
       "      <td>[0, 0, 4, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232708</td>\n",
       "      <td>USA</td>\n",
       "      <td>under 18</td>\n",
       "      <td>0345402871</td>\n",
       "      <td>Airframe</td>\n",
       "      <td>Michael Crichton</td>\n",
       "      <td>1997</td>\n",
       "      <td>Ballantine Books</td>\n",
       "      <td>[0425179559, 0425132935, 0425141551, 0425182886]</td>\n",
       "      <td>[10, 9, 10, 8]</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202783</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0345413350</td>\n",
       "      <td>The Golden Compass (His Dark Materials, Book 1)</td>\n",
       "      <td>PHILIP PULLMAN</td>\n",
       "      <td>1997</td>\n",
       "      <td>Del Rey</td>\n",
       "      <td>[0380789019, 0345413369, 0345413377, 0439049962]</td>\n",
       "      <td>[9, 5, 0, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116972</td>\n",
       "      <td>USA</td>\n",
       "      <td>40-44</td>\n",
       "      <td>0446611778</td>\n",
       "      <td>Last Man Standing</td>\n",
       "      <td>David Baldacci</td>\n",
       "      <td>2002</td>\n",
       "      <td>Warner Vision</td>\n",
       "      <td>[0440241073, 0345426274, 0380789035, 044660466...</td>\n",
       "      <td>[7, 0, 5, 8, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67174</td>\n",
       "      <td>USA</td>\n",
       "      <td>18-24</td>\n",
       "      <td>014100018X</td>\n",
       "      <td>Chocolat</td>\n",
       "      <td>Joanne Harris</td>\n",
       "      <td>2000</td>\n",
       "      <td>Penguin Books</td>\n",
       "      <td>[0553562614, 0449212602, 0553213695, 0886773520]</td>\n",
       "      <td>[9, 0, 6, 7]</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>265751</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0802130208</td>\n",
       "      <td>A Confederacy of Dunces (Evergreen Book)</td>\n",
       "      <td>John Kennedy Toole</td>\n",
       "      <td>1987</td>\n",
       "      <td>Grove Press</td>\n",
       "      <td>[068484477X, 0451166892, 044022103X, 068486574...</td>\n",
       "      <td>[10, 10, 9, 8, 7, 9, 10, 9, 8, 8]</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28735</td>\n",
       "      <td>Germany</td>\n",
       "      <td>55-59</td>\n",
       "      <td>0312971230</td>\n",
       "      <td>Ice Station</td>\n",
       "      <td>Matthew J. Reilly</td>\n",
       "      <td>2000</td>\n",
       "      <td>St. Martin's Press</td>\n",
       "      <td>[0440993717, 0671041789, 0971880107, 158322489...</td>\n",
       "      <td>[8, 0, 6, 8, 9, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>72352</td>\n",
       "      <td>Spain</td>\n",
       "      <td>45-49</td>\n",
       "      <td>0440110653</td>\n",
       "      <td>CRY IN THE NIGHT, A</td>\n",
       "      <td>MARY HIGGINS CLARK</td>\n",
       "      <td>1983</td>\n",
       "      <td>Dell</td>\n",
       "      <td>[067101417X, 0440237556, 0446364703, 0671797050]</td>\n",
       "      <td>[0, 0, 6, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>245864</td>\n",
       "      <td>USA</td>\n",
       "      <td>45-49</td>\n",
       "      <td>0553569058</td>\n",
       "      <td>The Robber Bride</td>\n",
       "      <td>Margaret Atwood</td>\n",
       "      <td>1995</td>\n",
       "      <td>Bantam</td>\n",
       "      <td>[0446606383, 0440235596, 0446329894, 042515863...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>79366</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0312963009</td>\n",
       "      <td>Neanderthal: A Novel</td>\n",
       "      <td>John Darnton</td>\n",
       "      <td>1997</td>\n",
       "      <td>St. Martin's Press</td>\n",
       "      <td>[0440241073, 0440241537, 0743411269, 097188010...</td>\n",
       "      <td>[5, 7, 6, 0, 0, 0, 8]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>86145</td>\n",
       "      <td>USA</td>\n",
       "      <td>45-49</td>\n",
       "      <td>0812580354</td>\n",
       "      <td>Calculating God</td>\n",
       "      <td>Robert J. Sawyer</td>\n",
       "      <td>2001</td>\n",
       "      <td>Tor Science Fiction</td>\n",
       "      <td>[0440111811, 0312963009, 0440170796, 067102837...</td>\n",
       "      <td>[0, 5, 0, 7, 0, 5, 0, 0, 0, 8, 9, 9, 7, 7, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>157273</td>\n",
       "      <td>USA</td>\n",
       "      <td>35-39</td>\n",
       "      <td>0553380958</td>\n",
       "      <td>Snow Crash (Bantam Spectra Book)</td>\n",
       "      <td>NEAL STEPHENSON</td>\n",
       "      <td>2000</td>\n",
       "      <td>Bantam</td>\n",
       "      <td>[0553280368, 0316779237, 0671027387, 006039245...</td>\n",
       "      <td>[0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>143134</td>\n",
       "      <td>USA</td>\n",
       "      <td>25-29</td>\n",
       "      <td>0618002219</td>\n",
       "      <td>The Hobbit: or There and Back Again</td>\n",
       "      <td>J.R.R. Tolkien</td>\n",
       "      <td>1999</td>\n",
       "      <td>Houghton Mifflin Company</td>\n",
       "      <td>[0452262143, 0440241073, 0380012863, 055328942...</td>\n",
       "      <td>[0, 10, 10, 10, 8, 10, 0, 0, 10, 6, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>57412</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>25-29</td>\n",
       "      <td>0385730586</td>\n",
       "      <td>Sisterhood of the Traveling Pants</td>\n",
       "      <td>ANN BRASHARES</td>\n",
       "      <td>2003</td>\n",
       "      <td>Delacorte Books for Young Readers</td>\n",
       "      <td>[0141301155, 037325024X, 0385504209, 031242215...</td>\n",
       "      <td>[0, 10, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>141450</td>\n",
       "      <td>USA</td>\n",
       "      <td>30-34</td>\n",
       "      <td>0140086838</td>\n",
       "      <td>Ceremony (Contemporary American Fiction Series)</td>\n",
       "      <td>Leslie Marmon Silko</td>\n",
       "      <td>1988</td>\n",
       "      <td>Penguin Books</td>\n",
       "      <td>[0786866586, 044651652X, 0670880728, 1573225517]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>242106</td>\n",
       "      <td>USA</td>\n",
       "      <td>25-29</td>\n",
       "      <td>0553287303</td>\n",
       "      <td>Second Child</td>\n",
       "      <td>John Saul</td>\n",
       "      <td>1997</td>\n",
       "      <td>Bantam Books</td>\n",
       "      <td>[015600710X, 0553561618, 0451409159, 068483597...</td>\n",
       "      <td>[0, 0, 0, 0, 10, 10, 10, 0, 0, 0, 0, 5, 10, 0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>105263</td>\n",
       "      <td>USA</td>\n",
       "      <td>18-24</td>\n",
       "      <td>0345354621</td>\n",
       "      <td>The Terminal Man</td>\n",
       "      <td>Michael Crichton</td>\n",
       "      <td>1988</td>\n",
       "      <td>Ballantine Books</td>\n",
       "      <td>[0425083837, 0446365505, 059035342X, 043906487...</td>\n",
       "      <td>[0, 0, 10, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9226</td>\n",
       "      <td>USA</td>\n",
       "      <td>25-29</td>\n",
       "      <td>0312263120</td>\n",
       "      <td>Digital Fortress : A Thriller</td>\n",
       "      <td>Dan Brown</td>\n",
       "      <td>2000</td>\n",
       "      <td>St. Martin's Griffin</td>\n",
       "      <td>[0345443683, 0451183665, 0671027360, 038531709...</td>\n",
       "      <td>[9, 0, 0, 7, 0, 8, 0, 9]</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>251242</td>\n",
       "      <td>Canada</td>\n",
       "      <td>25-29</td>\n",
       "      <td>0345358791</td>\n",
       "      <td>2061: Odyssey Three</td>\n",
       "      <td>Arthur C. Clarke</td>\n",
       "      <td>1991</td>\n",
       "      <td>Del Rey Books</td>\n",
       "      <td>[0451162072, 0451161351, 0345391055, 044900241...</td>\n",
       "      <td>[7, 5, 0, 7, 8, 7, 9, 8, 8, 0, 9, 8, 10, 9, 0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User ID  Location       Age        ISBN  \\\n",
       "0   242790       USA   unknown  0312976275   \n",
       "1   205059     India       60+  0099244926   \n",
       "2   232708       USA  under 18  0345402871   \n",
       "3   202783       USA   unknown  0345413350   \n",
       "4   116972       USA     40-44  0446611778   \n",
       "5    67174       USA     18-24  014100018X   \n",
       "6   265751       USA   unknown  0802130208   \n",
       "7    28735   Germany     55-59  0312971230   \n",
       "8    72352     Spain     45-49  0440110653   \n",
       "9   245864       USA     45-49  0553569058   \n",
       "10   79366       USA   unknown  0312963009   \n",
       "11   86145       USA     45-49  0812580354   \n",
       "12  157273       USA     35-39  0553380958   \n",
       "13  143134       USA     25-29  0618002219   \n",
       "14   57412  Portugal     25-29  0385730586   \n",
       "15  141450       USA     30-34  0140086838   \n",
       "16  242106       USA     25-29  0553287303   \n",
       "17  105263       USA     18-24  0345354621   \n",
       "18    9226       USA     25-29  0312263120   \n",
       "19  251242    Canada     25-29  0345358791   \n",
       "\n",
       "                                           Book title               Author  \\\n",
       "0   Hot Six : A Stephanie Plum Novel (A Stephanie ...      Janet Evanovich   \n",
       "1                                   The Street Lawyer         John Grisham   \n",
       "2                                            Airframe     Michael Crichton   \n",
       "3     The Golden Compass (His Dark Materials, Book 1)       PHILIP PULLMAN   \n",
       "4                                   Last Man Standing       David Baldacci   \n",
       "5                                            Chocolat        Joanne Harris   \n",
       "6            A Confederacy of Dunces (Evergreen Book)   John Kennedy Toole   \n",
       "7                                         Ice Station    Matthew J. Reilly   \n",
       "8                                 CRY IN THE NIGHT, A   MARY HIGGINS CLARK   \n",
       "9                                    The Robber Bride      Margaret Atwood   \n",
       "10                               Neanderthal: A Novel         John Darnton   \n",
       "11                                    Calculating God     Robert J. Sawyer   \n",
       "12                   Snow Crash (Bantam Spectra Book)      NEAL STEPHENSON   \n",
       "13                The Hobbit: or There and Back Again       J.R.R. Tolkien   \n",
       "14                  Sisterhood of the Traveling Pants        ANN BRASHARES   \n",
       "15    Ceremony (Contemporary American Fiction Series)  Leslie Marmon Silko   \n",
       "16                                       Second Child            John Saul   \n",
       "17                                   The Terminal Man     Michael Crichton   \n",
       "18                      Digital Fortress : A Thriller            Dan Brown   \n",
       "19                                2061: Odyssey Three     Arthur C. Clarke   \n",
       "\n",
       "   Publication year                          Publisher  \\\n",
       "0              2001            St. Martin's Paperbacks   \n",
       "1              1999                Random House Uk Ltd   \n",
       "2              1997                   Ballantine Books   \n",
       "3              1997                            Del Rey   \n",
       "4              2002                      Warner Vision   \n",
       "5              2000                      Penguin Books   \n",
       "6              1987                        Grove Press   \n",
       "7              2000                 St. Martin's Press   \n",
       "8              1983                               Dell   \n",
       "9              1995                             Bantam   \n",
       "10             1997                 St. Martin's Press   \n",
       "11             2001                Tor Science Fiction   \n",
       "12             2000                             Bantam   \n",
       "13             1999           Houghton Mifflin Company   \n",
       "14             2003  Delacorte Books for Young Readers   \n",
       "15             1988                      Penguin Books   \n",
       "16             1997                       Bantam Books   \n",
       "17             1988                   Ballantine Books   \n",
       "18             2000               St. Martin's Griffin   \n",
       "19             1991                      Del Rey Books   \n",
       "\n",
       "                                            user_hist  \\\n",
       "0    [0449003787, 0312980140, 0425155404, 0449003795]   \n",
       "1    [0449227421, 0440211727, 0440236673, 0449227545]   \n",
       "2    [0425179559, 0425132935, 0425141551, 0425182886]   \n",
       "3    [0380789019, 0345413369, 0345413377, 0439049962]   \n",
       "4   [0440241073, 0345426274, 0380789035, 044660466...   \n",
       "5    [0553562614, 0449212602, 0553213695, 0886773520]   \n",
       "6   [068484477X, 0451166892, 044022103X, 068486574...   \n",
       "7   [0440993717, 0671041789, 0971880107, 158322489...   \n",
       "8    [067101417X, 0440237556, 0446364703, 0671797050]   \n",
       "9   [0446606383, 0440235596, 0446329894, 042515863...   \n",
       "10  [0440241073, 0440241537, 0743411269, 097188010...   \n",
       "11  [0440111811, 0312963009, 0440170796, 067102837...   \n",
       "12  [0553280368, 0316779237, 0671027387, 006039245...   \n",
       "13  [0452262143, 0440241073, 0380012863, 055328942...   \n",
       "14  [0141301155, 037325024X, 0385504209, 031242215...   \n",
       "15   [0786866586, 044651652X, 0670880728, 1573225517]   \n",
       "16  [015600710X, 0553561618, 0451409159, 068483597...   \n",
       "17  [0425083837, 0446365505, 059035342X, 043906487...   \n",
       "18  [0345443683, 0451183665, 0671027360, 038531709...   \n",
       "19  [0451162072, 0451161351, 0345391055, 044900241...   \n",
       "\n",
       "                                          hist_rating  labels  rating  \n",
       "0                                       [6, 10, 0, 9]       0       0  \n",
       "1                                        [0, 0, 4, 0]       0       0  \n",
       "2                                      [10, 9, 10, 8]       1      10  \n",
       "3                                        [9, 5, 0, 5]       0       0  \n",
       "4                                     [7, 0, 5, 8, 0]       0       0  \n",
       "5                                        [9, 0, 6, 7]       1       9  \n",
       "6                   [10, 10, 9, 8, 7, 9, 10, 9, 8, 8]       1       7  \n",
       "7                               [8, 0, 6, 8, 9, 0, 0]       0       0  \n",
       "8                                        [0, 0, 6, 0]       1       7  \n",
       "9   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, ...       0       0  \n",
       "10                              [5, 7, 6, 0, 0, 0, 8]       0       0  \n",
       "11  [0, 5, 0, 7, 0, 5, 0, 0, 0, 8, 9, 9, 7, 7, 0, ...       1       8  \n",
       "12  [0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, ...       0       0  \n",
       "13             [0, 10, 10, 10, 8, 10, 0, 0, 10, 6, 0]       1      10  \n",
       "14                                [0, 10, 0, 0, 0, 0]       0       0  \n",
       "15                                       [0, 0, 0, 0]       0       0  \n",
       "16  [0, 0, 0, 0, 10, 10, 10, 0, 0, 0, 0, 5, 10, 0,...       0       0  \n",
       "17                          [0, 0, 10, 0, 0, 0, 0, 0]       0       0  \n",
       "18                           [9, 0, 0, 7, 0, 8, 0, 9]       1       8  \n",
       "19  [7, 5, 0, 7, 8, 7, 9, 8, 8, 0, 9, 8, 10, 9, 0,...       1       8  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "random.shuffle(data_list)\n",
    "df_data = pd.DataFrame(data_list, columns=user_fields + book_fields + [\"user_hist\", \"hist_rating\" , \"labels\", \"rating\"])\n",
    "print(f\"Total number of samples: {len(df_data)}\")\n",
    "\n",
    "df_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train num: 7850\n",
      "Test num: 873\n"
     ]
    }
   ],
   "source": [
    "# Save train/test in parquet format\n",
    "\n",
    "df_train = df_data[:int(0.9 * len(df_data))].reset_index(drop=True)\n",
    "df_test = df_data[int(0.9 * len(df_data)):].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train num: {len(df_train)}\")\n",
    "print(f\"Test num: {len(df_test)}\")\n",
    "\n",
    "df_train.to_parquet(\n",
    "    os.path.join(target_dir, \"train.parquet.gz\"), \n",
    "    compression=\"gzip\", \n",
    ")\n",
    "df_test.to_parquet(\n",
    "    os.path.join(target_dir, \"test.parquet.gz\"), \n",
    "    compression=\"gzip\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-read for sanity check\n",
    "\n",
    "train_dataset = pd.read_parquet(os.path.join(target_dir, \"train.parquet.gz\"))\n",
    "test_dataset = pd.read_parquet(os.path.join(target_dir, \"test.parquet.gz\"))\n",
    "\n",
    "for (i1, a1), (i2, a2) in zip(df_train.iterrows(), train_dataset.iterrows()):\n",
    "    for field in user_fields + book_fields + [\"labels\"]:\n",
    "        assert not isinstance(a1[field], str) or \"\\t\" not in a1[field]\n",
    "        assert a1[field] == a2[field], (field, a1[field], a2[field])\n",
    "for (i1, a1), (i2, a2) in zip(df_test.iterrows(), test_dataset.iterrows()):\n",
    "    for field in user_fields + book_fields + [\"labels\"]:\n",
    "        assert not isinstance(a1[field], str) or \"\\t\" not in a1[field]\n",
    "        assert a1[field] == a2[field], (field, a1[field], a2[field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID 8723\n",
      "Location 109\n",
      "Age 11\n",
      "ISBN 3547\n",
      "Book title 3164\n",
      "Author 1199\n",
      "Publication year 44\n",
      "Publisher 310\n"
     ]
    }
   ],
   "source": [
    "# Save the meta data for CTR\n",
    "\n",
    "field_names = user_fields + book_fields\n",
    "\n",
    "feature_count = [len(feature_dict[field]) for field in field_names]\n",
    "\n",
    "feature_offset = [0]\n",
    "for c in feature_count[:-1]:\n",
    "    feature_offset.append(feature_offset[-1] + c)\n",
    "\n",
    "for field in field_names:\n",
    "    print(field, len(feature_dict[field]))\n",
    "\n",
    "meta_data = {\n",
    "    'field_names': field_names,\n",
    "    'feature_count': feature_count,\n",
    "    'feature_dict': feature_dict,\n",
    "    'feature_offset': feature_offset,\n",
    "    'num_ratings': 11\n",
    "}\n",
    "\n",
    "json.dump(meta_data, open(os.path.join(target_dir, 'ctr-meta.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dict = json.load(open(os.path.join(target_dir, 'book_dict.json')))\n",
    "meta_data = json.load(open(os.path.join(target_dir, 'ctr-meta.json')))\n",
    "isbn2id = meta_data['feature_dict']['ISBN']\n",
    "id2book = {book_id: [isbn] + book_dict[isbn] for isbn, book_id in isbn2id.items()}\n",
    "json.dump(id2book, open(os.path.join(target_dir, 'id2book_1.json'), \"w\"), indent=4)\n",
    "json.dump(isbn2id, open(os.path.join(target_dir, 'isbn2id_1.json'), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctr_X (8723, 8)\n",
      "ctr_Y (8723,)\n"
     ]
    }
   ],
   "source": [
    "# Convert df_data to CTR data via feature_dict\n",
    "\n",
    "ctr_X, ctr_Y = [], []\n",
    "for idx, row in df_data.iterrows():\n",
    "    ctr_X.append([feature_dict[field][row[field]] for field in field_names])\n",
    "    ctr_Y.append(int(row[\"labels\"]))\n",
    "\n",
    "\n",
    "ctr_X = np.array(ctr_X)\n",
    "ctr_Y = np.array(ctr_Y)\n",
    "print(\"ctr_X\", ctr_X.shape)\n",
    "print(\"ctr_Y\", ctr_Y.shape)\n",
    "feature_count_np = np.array(feature_count).reshape(1, -1)\n",
    "assert (ctr_X - feature_count_np <= 0).sum() == ctr_X.shape[0] * ctr_X.shape[1]\n",
    "assert (ctr_Y == 0).sum() + (ctr_Y == 1).sum() == ctr_Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_column = {}\n",
    "\n",
    "# history_column[\"ID\"] = df_data['user_hist'].tolist()\n",
    "history_column[\"ID\"] = [[isbn2id[x] for x in hist] for hist in df_data['user_hist'].tolist()]\n",
    "history_column[\"rating\"] = df_data['hist_rating'].tolist()\n",
    "history_column[\"hist length\"] = [len(x) for x in history_column[\"rating\"]]\n",
    "\n",
    "train_num = int(0.9 * len(ctr_X))\n",
    "\n",
    "user_seq = {\n",
    "    \"history ID\": {\n",
    "        \"train\": history_column[\"ID\"][:train_num],\n",
    "        \"test\": history_column[\"ID\"][train_num:],\n",
    "    },\n",
    "    \"history rating\": {\n",
    "        \"train\": history_column[\"rating\"][:train_num],\n",
    "        \"test\": history_column[\"rating\"][train_num:],\n",
    "    },\n",
    "    \"history length\": {\n",
    "        \"train\": history_column[\"hist length\"][:train_num],\n",
    "        \"test\": history_column[\"hist length\"][train_num:],\n",
    "    },\n",
    "}\n",
    "\n",
    "json.dump(user_seq, open(os.path.join(target_dir, \"user_seq.json\"), \"w\"), ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "873"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_seq[\"history ID\"][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history ID train torch.Size([7850, 60])\n",
      "history ID test torch.Size([873, 60])\n",
      "history rating train torch.Size([7850, 60])\n",
      "history rating test torch.Size([873, 60])\n",
      "history mask train torch.Size([7850, 60])\n",
      "history mask test torch.Size([873, 60])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "user_seq_trunc = {\n",
    "    \"history ID\": {}, \n",
    "    \"history rating\": {}, \n",
    "    \"history mask\": {}, \n",
    "}\n",
    "\n",
    "for hist_name in user_seq:\n",
    "    for split in user_seq[hist_name]:\n",
    "        if hist_name != \"history length\":\n",
    "            user_seq_trunc[hist_name][split] = pad_sequence(\n",
    "                [torch.tensor(x[-60:]) for x in user_seq[hist_name][split]], \n",
    "                batch_first=True, \n",
    "            )\n",
    "        else:\n",
    "            user_seq_trunc[\"history mask\"][split] = pad_sequence(\n",
    "                [torch.ones(min(x, 60)) for x in user_seq[hist_name][split]], \n",
    "                batch_first=True, \n",
    "            )\n",
    "\n",
    "md5_user_seq_trunc = {}\n",
    "for hist_name in user_seq_trunc:\n",
    "    md5_user_seq_trunc[hist_name] = {}\n",
    "    for split in user_seq_trunc[hist_name]:\n",
    "        md5_user_seq_trunc[hist_name][split] = user_seq_trunc[hist_name][split].tolist()\n",
    "        print(hist_name, split, user_seq_trunc[hist_name][split].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (7850, 8)\n",
      "test data: (873, 8)\n",
      "train label: (7850,)\n",
      "test label: (873,)\n"
     ]
    }
   ],
   "source": [
    "# Save CTR data\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f'ctr.h5'), 'w') as hf:\n",
    "    hf.create_dataset('train data', data=ctr_X[:int(0.9 * len(ctr_X)), :])\n",
    "    hf.create_dataset('test data', data=ctr_X[int(0.9 * len(ctr_X)):, :])\n",
    "    hf.create_dataset('train label', data=ctr_Y[:int(0.9 * len(ctr_X))])\n",
    "    hf.create_dataset('test label', data=ctr_Y[int(0.9 * len(ctr_X)):])\n",
    "    for hist_name in user_seq_trunc:\n",
    "        for split in user_seq_trunc[hist_name]:\n",
    "            hf.create_dataset(f\"{split} {hist_name}\", data=user_seq_trunc[hist_name][split])\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f'ctr.h5'), 'r') as hf:\n",
    "    assert (ctr_X - np.concatenate([hf['train data'][:], hf['test data'][:]], axis=0)).sum() == 0\n",
    "    assert (ctr_Y - np.concatenate([hf['train label'][:], hf['test label'][:]], axis=0)).sum() == 0\n",
    "    for hist_name in user_seq_trunc:\n",
    "        for split in user_seq_trunc[hist_name]:\n",
    "            assert (user_seq_trunc[hist_name][split] - hf[f\"{split} {hist_name}\"][:]).sum() == 0    \n",
    "\n",
    "    x = hf['train data'][:]\n",
    "    assert (x - ctr_X[:int(0.9 * len(ctr_X)), :]).sum() == 0\n",
    "    print(f'train data: {x.shape}')\n",
    "    \n",
    "    x = hf['test data'][:]\n",
    "    assert (x - ctr_X[int(0.9 * len(ctr_X)):, :]).sum() == 0\n",
    "    print(f'test data: {x.shape}')\n",
    "    x = hf['train label'][:]\n",
    "    assert (x - ctr_Y[:int(0.9 * len(ctr_X))]).sum() == 0\n",
    "    print(f'train label: {x.shape}')\n",
    "    x = hf['test label'][:]\n",
    "    assert (x - ctr_Y[int(0.9 * len(ctr_X)):]).sum() == 0\n",
    "    print(f'test label: {x.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass final check.\n"
     ]
    }
   ],
   "source": [
    "# Final check: ensure each row from tsv and ctr is matched\n",
    "\n",
    "train_dataset = pd.read_parquet(os.path.join(target_dir, 'train.parquet.gz'))\n",
    "test_dataset = pd.read_parquet(os.path.join(target_dir, 'test.parquet.gz')).reset_index(drop=True)\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f'ctr.h5'), 'r') as hf:\n",
    "    train_x = hf['train data'][:]\n",
    "    train_y = hf['train label'][:]\n",
    "    test_x = hf['test data'][:]\n",
    "    test_y = hf['test label'][:]\n",
    "\n",
    "for idx, row in train_dataset.iterrows():\n",
    "    for fi, field in enumerate(field_names):\n",
    "        assert feature_dict[field][row[field]] == train_x[idx, fi]\n",
    "    assert int(row[\"labels\"]) == train_y[idx]\n",
    "\n",
    "for idx, row in test_dataset.iterrows():\n",
    "    for fi, field in enumerate(field_names):\n",
    "        assert feature_dict[field][row[field]] == test_x[idx, fi]\n",
    "    assert int(row[\"labels\"]) == test_y[idx]\n",
    "\n",
    "print(\"Pass final check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set shape: (227735, 3)\n"
     ]
    }
   ],
   "source": [
    "## split for training recommend models\n",
    "rating_data = []\n",
    "rating_fields = [\"User ID\", \"ISBN\", \"rating\"]\n",
    "for user, isbn, rating in processed_list:\n",
    "    if user in feature_dict[\"User ID\"] and isbn in feature_dict[\"ISBN\"]:\n",
    "        rating_data.append([user, isbn, rating])\n",
    "\n",
    "df_ratings = pd.DataFrame(rating_data, columns=rating_fields)\n",
    "print(\"Data set shape:\", df_ratings.shape)\n",
    "\n",
    "# Sort by user_id and timestamp\n",
    "df_ratings = df_ratings.sort_values(by=['User ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>227735</td>\n",
       "      <td>227735</td>\n",
       "      <td>227735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8723</td>\n",
       "      <td>3547</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>11676</td>\n",
       "      <td>0971880107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2117</td>\n",
       "      <td>1129</td>\n",
       "      <td>147856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User ID        ISBN  rating\n",
       "count   227735      227735  227735\n",
       "unique    8723        3547      11\n",
       "top      11676  0971880107       0\n",
       "freq      2117        1129  147856"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3547"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_counts = df_ratings['ISBN'].value_counts()\n",
    "multi_occurrences = book_counts[book_counts >= 5].index\n",
    "len(multi_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (220403, 3)\n",
      "Test set shape: (7332, 3)\n"
     ]
    }
   ],
   "source": [
    "# Group by each user and filter for users with more than 5 ratings\n",
    "user_groups = df_ratings.groupby('User ID').filter(lambda x: len(x) > 5)\n",
    "\n",
    "# Extract the last record for each user in the test set and the rest as training\n",
    "tail_set = user_groups.groupby('User ID').tail(1)\n",
    "test_set = tail_set[tail_set['ISBN'].isin(multi_occurrences)]\n",
    "train_set = df_ratings.drop(test_set.index)\n",
    "\n",
    "# Display the shapes to confirm the split\n",
    "print(\"Training set shape:\", train_set.shape)\n",
    "print(\"Test set shape:\", test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as .txt files\n",
    "df_train_rec = train_set.sort_values(by=[\"User ID\", \"ISBN\"], inplace=False, kind=\"stable\")\n",
    "df_test_rec = test_set.sort_values(by=[\"User ID\", \"ISBN\"], inplace=False, kind=\"stable\")\n",
    "df_train_rec.to_csv(os.path.join(target_dir, \"train.txt\"), sep=' ', index=False, header=None)\n",
    "df_test_rec.to_csv(os.path.join(target_dir, \"test.txt\"), sep=' ', index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
